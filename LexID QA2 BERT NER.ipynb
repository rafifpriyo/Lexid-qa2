{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNfEI7ROMUKEt3fK2pKxV4O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{"id":"VLyFRr32FJVI"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import DataCollatorForTokenClassification\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","# from torchsummary import summary"],"metadata":{"id":"ExwS2qj3wbro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710311883732,"user_tz":-420,"elapsed":9514,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"0311772c-cfd2-4628-9686-f62e76faee53"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\Rofif\\anaconda3\\envs\\lexidvenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}]},{"cell_type":"markdown","source":["## Define NER Label"],"metadata":{"id":"b-EcNk6fE_42"}},{"cell_type":"code","source":["# Info: Transformers (huggingface) is quite different from pytorch\n","label_names = [\"O\", \"B-LEG\", \"I-LEG\", \"B-NUM\", \"I-NUM\",\n","                   \"B-YER\", \"I-YER\", \"B-PAS\", \"I-PAS\", \"B-AYT\", \"I-AYT\"]\n","id2label = {i: label for i, label in enumerate(label_names)}\n","label2id = {v: k for k, v in id2label.items()}"],"metadata":{"id":"fPXn0VEiFBxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Download Model"],"metadata":{"id":"a_6Eh6efwcPA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvXoziDAvBrW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710253979740,"user_tz":-420,"elapsed":3440,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"5a6afc05-9f93-4e3c-d977-e332c4bbc11c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n","# model_ner = AutoModelForTokenClassification.from_pretrained(\"indolem/indobert-base-uncased\", id2label=id2label, label2id=label2id)\n","\n","# Data collator is used for padding in batch\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"markdown","source":["## Save Model to local - Example"],"metadata":{"id":"8LWDtdFCyIVj"}},{"cell_type":"code","source":["save_path = 'model/'\n","\n","model_ner_path = os.path.join(save_path, \"indobert_ner\"+\".pth\")\n","\n","torch.save(model_ner, model_ner_path)"],"metadata":{"id":"EkYP_H3lxmHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Model - Example"],"metadata":{"id":"rToLnmrQyGFz"}},{"cell_type":"code","source":["save_path = 'model/'\n","model_ner_path = os.path.join(save_path, \"indobert_ner\"+\".pth\")\n","\n","bert_ner = torch.load(model_ner_path)"],"metadata":{"id":"Ig8Ser7cyBPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_ner.config.num_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnF6h0FGFZ-L","executionInfo":{"status":"ok","timestamp":1710254046991,"user_tz":-420,"elapsed":9,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"55161100-e038-4639-c76d-809737b94989"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(bert_ner)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjs42HOaFu6o","executionInfo":{"status":"ok","timestamp":1710253990283,"user_tz":-420,"elapsed":18,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"704eae87-f50c-4d72-9145-cbae4c372e49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31923, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["## Bert Test Simple Input"],"metadata":{"id":"Jd4cZU4pnLAU"}},{"cell_type":"code","source":["input = tokenizer(\"apa bedanya biru sama merah?\")\n","print(input.word_ids())\n","output = bert_ner(torch.tensor(input['input_ids']).view(1, -1), return_dict=False)\n","output[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7lqZAS29Eun","executionInfo":{"status":"ok","timestamp":1710250087069,"user_tz":-420,"elapsed":1165,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"d235d7bb-38c7-4d5c-cc70-28f8017d5556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[None, 0, 1, 2, 3, 4, 5, None]\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.5823, -0.3254],\n","         [ 0.1367,  0.2392],\n","         [ 0.1105, -0.1016],\n","         [ 0.0899, -0.0853],\n","         [ 0.2738, -0.1482],\n","         [ 0.2239, -0.1915],\n","         [ 0.4310, -0.4585],\n","         [ 0.3281, -0.6050]]], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# LexID QA 2 Using BERT"],"metadata":{"id":"uKvt0oSQnsJO"}},{"cell_type":"markdown","source":["## Dataset NER"],"metadata":{"id":"y-TBgMJPoOEZ"}},{"cell_type":"code","source":["class Lexidqa2Dataset(Dataset):\n","  \"\"\"LexID QA2 Dataset\"\"\"\n","\n","  def __init__(self, dataframe, tokenizer):\n","    \"\"\"\n","      dataframe (Pandas DataFrame): Dataset in DataFrame format\n","      tokenizer (Tokenizer) : Sentence to Vector\n","    \"\"\"\n","    self.qa_frame = dataframe\n","    self.tokenizer = tokenizer\n","\n","    self.label2id = {\"O\": 0, \"B-LEG\": 1, \"I-LEG\": 2\n","                \"B-NUM\": 3, \"I-NUM\": 4, \"B-YER\": 5, \"I-YER\": 6,\n","                \"B-PAS\": 7, \"I-PAS\": 8, \"B-AYT\": 9, \"I-AYT\": 10}\n","    self.id2label = {0: \"O\", 1: \"B-LEG\", 2: \"I-LEG\",\n","                3: \"B-NUM\", 4: \"I-NUM\", 5: \"B-YER\", 6: \"I-YER\",\n","                7: \"B-PAS\", 8: \"I-PAS\", 9: \"B-AYT\", 10: \"I-AYT\"}\n","\n","  def __len__(self):\n","    return len(self.qa_frame)\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","\n","    question = self.qa_frame.loc[idx, \"question\"]\n","    question_tokenized = tokenizer(question)\n","    question_input_ids = torch.tensor(question_tokenized['input_ids'])\n","    question_token_type_ids = torch.tensor(question_tokenized['token_type_ids'])\n","    question_attention_masks = torch.tensor(question_tokenized['attention_mask'])\n","\n","    answer = self.qa_frame.loc[idx, \"answer\"]\n","\n","    ner_labels = self.qa_frame.loc[idx, \"ner\"]\n","    ner_labels = self.align_labels_with_tokens(ner_labels, question_tokenized.word_ids())\n","\n","    sample = {'question': question,\n","              'question_input_ids': question_input_ids,\n","              'question_token_type_ids': question_token_type_ids,\n","              'question_attention_masks': question_attention_masks,\n","              'answer': answer}\n","\n","    return sample\n","\n","  def one_hot(self, label, label2id):\n","    one_hot_label = torch.zeros(len(label2id))\n","    one_hot_label[label2id[label]] = 1\n","\n","    return one_hot_label\n","\n","  def align_labels_with_tokens(labels, word_ids):\n","    new_labels = []\n","    current_word = None\n","    for word_id in word_ids:\n","        if word_id != current_word:\n","            # Start of a new word!\n","            current_word = word_id\n","            label = -100 if word_id is None else labels[word_id]\n","            new_labels.append(label)\n","        elif word_id is None:\n","            # Special token\n","            new_labels.append(-100)\n","        else:\n","            # Same word as previous token\n","            # label = labels[word_id]\n","            # # If the label is B-XXX we change it to I-XXX\n","            # if label % 2 == 1:\n","            #     label += 1\n","            # new_labels.append(label)\n","            new_labels.append(-100)\n","\n","    return new_labels"],"metadata":{"id":"tYVmWC9EoPu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Merging Dataset from Different csv"],"metadata":{"id":"ZRCQzc_prgOT"}},{"cell_type":"code","source":["t1 = pd.read_csv(\"t1_dataset.csv\")\n","t2 = pd.read_csv(\"t2_dataset.csv\")\n","t3 = pd.read_csv(\"t3_dataset.csv\")\n","t4 = pd.read_csv(\"t4_dataset.csv\")\n","t5 = pd.read_csv(\"t5_dataset.csv\")\n","t6 = pd.read_csv(\"t6_dataset.csv\")\n","t7 = pd.read_csv(\"t7_dataset.csv\")\n","t8 = pd.read_csv(\"t8_dataset.csv\")\n","t10 = pd.read_csv(\"t10_dataset.csv\")\n","t11 = pd.read_csv(\"t11_dataset.csv\")\n","t12 = pd.read_csv(\"t12_dataset.csv\")\n","\n","t = pd.concat([t1, t2, t3, t4, t5, t6, t7, t8, t10, t11, t12], ignore_index=True)"],"metadata":{"id":"RtX4s-Lyrdev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"VawH8WeQtDbn"}},{"cell_type":"code","source":["qa_dataset = Lexidqa2Dataset(t, tokenizer)"],"metadata":{"id":"SKnksrAhtJ-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sample"],"metadata":{"id":"uHx7nN2PtpHN"}},{"cell_type":"code","source":["np.random.seed(12345)\n","random_indices = np.random.choice(range(len(qa_dataset)), size=5, replace=False)\n","\n","for i in random_indices:\n","  sample = qa_dataset[i]\n","\n","  print(i, sample['question'])\n","\n","  if i == 5:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZw3QiEotoBJ","executionInfo":{"status":"ok","timestamp":1709076159257,"user_tz":-420,"elapsed":20,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"5d82a67b-4678-4580-df4b-b8e5e026d1e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16250 Pada tanggal berapa Peraturan Presiden Republik Indonesia Nomor 59 Tahun 2005 tersebut diresmikan\n","69433 Peraturan Menteri Riset Teknologi Dan Pendidikan Tinggi Republik Indonesia Nomor 54 Tahun 2016 diadakan dengan fokus\n","96674 Peraturan Daerah Kabupaten Aceh Tamiang Nomor 20 Tahun 2011 dibangun dengan landasan hukum apa saja\n","96162 Peraturan Mahkamah Agung Republik Indonesia Nomor 8 Tahun 2017 dirancang berdasarkan basis apa\n","22872 Tanggal berapa penerbitan Peraturan Presiden Republik Indonesia Nomor 100 Tahun 2018 ini\n"]}]},{"cell_type":"markdown","source":["## Split, DataLoader"],"metadata":{"id":"RN6bW00QvBUT"}},{"cell_type":"code","source":["torch.manual_seed(12345)\n","train_size = int(0.5 * len(qa_dataset))\n","val_size = int(0.3 * len(qa_dataset))\n","test_size = len(qa_dataset) - train_size - val_size\n","train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(qa_dataset, [train_size, val_size, test_size])"],"metadata":{"id":"__sJBP1svEzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","\n","train_instances = DataLoader(train_dataset, collate_fn=data_collator, batch_size=BATCH_SIZE, shuffle=True)\n","valid_instances = DataLoader(valid_dataset, collate_fn=data_collator, batch_size=BATCH_SIZE, shuffle=False)\n","test_instances = DataLoader(test_dataset, collate_fn=data_collator, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"id":"cfxa1rpovc5b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# NER"],"metadata":{"id":"PVxd_Q0N9XXH"}},{"cell_type":"markdown","source":["References:\n","\n","- https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt"],"metadata":{"id":"AowgBs00_3mM"}},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"x_JiLxjlwDmX"}},{"cell_type":"markdown","source":["### Loss Function"],"metadata":{"id":"4QpZnDHJwF5Y"}},{"cell_type":"markdown","metadata":{"id":"_c2JSgTKwgXS"},"source":["> Loss Function Multi-Class Classification: CrossEntropyLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7s8t4IY0pMj"},"outputs":[],"source":["loss_func = torch.nn.CrossEntropyLoss(reduction='mean')"]},{"cell_type":"markdown","source":["### BERT"],"metadata":{"id":"MyzHs-k0nvXG"}},{"cell_type":"code","source":["class BERT_Lexidqa2_ner(nn.Module):\n","    def __init__(self, bert_for_token_classification, input_size = 768, output_size = 17):\n","        super().__init__()\n","\n","        self.bert = bert_for_token_classification\n","\n","    def forward(self, x):\n","        x = self.bert(x, return_dict=False)\n","\n","        return x"],"metadata":{"id":"WBYXgd3Snp9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERT_Lexidqa2_ner(bert_ner, 768, 17)"],"metadata":{"id":"uAm_ZnAc11ui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(qa_dataset[0]['question_input_ids'].unsqueeze_(0)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBRGyKbx2ikR","executionInfo":{"status":"ok","timestamp":1709080626766,"user_tz":-420,"elapsed":69,"user":{"displayName":"Rofif Priyo","userId":"07945053337059741601"}},"outputId":"d932b1b8-fdf1-4d0a-ee43-f96e28b90154"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 17])"]},"metadata":{},"execution_count":203}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"IClDr7hVXCax"}},{"cell_type":"code","source":["# training image classification\n","\n","LEARNING_RATE = 5e-5\n","EPOCH = 30\n","\n","# optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n","\n","for epoch_i in range(0, EPOCH):\n","  print(f\"Epoch {epoch_i + 1} / {EPOCH}\")\n","  # Reset the total loss for this epoch.\n","  tr_loss = 0\n","\n","  # Put the model into training mode.\n","  model.train()\n","\n","  # Untuk setiap data di training data\n","  for data in train_instances:\n","\n","    # Mendapatkan output dari model\n","    pred = model(data['image'])\n","\n","    # Categorical Cross-Entropy --- Loss Function\n","    loss = loss_func(pred, data['genre'])\n","\n","    # Agar gradient tidak menumpuk; ini diperlukan sebelum\n","    # menghitung gradient dengan loss.backward()\n","    optimizer.zero_grad()\n","\n","    # hitung gradient\n","    loss.backward()\n","\n","    # update parameter\n","    optimizer.step()\n","\n","    # akumulasi loss dalam 1 epoch\n","    tr_loss += loss.item()\n","\n","  # rata-rata loss dalam 1 epoch\n","  avg_loss = tr_loss / train_size\n","\n","  print(f\"Average loss: {avg_loss}\")\n","\n","  ### Uji coba di Validation Data\n","  print(\"Uji coba di Validation Data ...\")\n","\n","  # model eval, agar layer seperti dropout yang menghasilkan\n","  # sesuai yang random tidak digunakan\n","  model.eval()\n","\n","  val_loss = 0\n","  pred_labels = []\n","  true_labels = []\n","\n","  # Untuk setiap batch di validation data\n","  for data in valid_instances:\n","\n","    # jangan track gradient! ini sedang evaluasi, bukan training\n","    with torch.no_grad():\n","\n","      # Mendapatkan output dari model\n","      pred = model(data['image'])\n","\n","      # Categorical Cross-Entropy --- Loss Function\n","      loss = loss_func(pred, data['genre'])\n","\n","      # akumulasi loss di validation set\n","      val_loss += loss.item()\n","\n","    # akumulasi prediksi\n","    # _, pred_l = torch.max(output, dim = 1) # outnya adalah 2-tuple: (max, max_indices)\n","    # pred_labels += pred_l.detach().cpu()\n","    # _, true_l = torch.max(y, dim = 1)\n","    # true_labels += true_l.detach().cpu()\n","\n","  # average loss\n","  avg_val_loss = val_loss / val_size\n","\n","  # accuracy\n","  # pred_labels = torch.stack(pred_labels).numpy()\n","  # true_labels = torch.stack(true_labels).numpy()\n","  # val_accuracy = np.sum(pred_labels == true_labels) / len(pred_labels)\n","  print(f\"Average loss di validation data: {avg_val_loss}\")\n","  # print(f\"Accuracy di validation data: {val_accuracy}\")\n","\n","  print(\"\")\n","\n","  if epoch_i % 5:\n","    save_path = 'model/ner_checkpoint/'\n","    model_ner_path = os.path.join(save_path, \"indobert_ner_checkpoint_\" + epoch_i +\".pth\")\n","    torch.save(model_ner, model_ner_path)"],"metadata":{"id":"Y1Ux6MNxXD5E"},"execution_count":null,"outputs":[]}]}